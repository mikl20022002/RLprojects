{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "95523f66-5665-457f-b32e-95c4e6485d0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gymnasium as gym\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.optim import Adam\n",
    "from torch.distributions import Categorical\n",
    "import numpy as np\n",
    "import tyro\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from dataclasses import dataclass\n",
    "import tensorboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3d2c273f-aadf-4f88-b80d-72a97edf4613",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "500\n",
      "1000\n",
      "1500\n",
      "2000\n",
      "2500\n",
      "3000\n",
      "3500\n",
      "4000\n",
      "4500\n",
      "5000\n",
      "5500\n",
      "6000\n",
      "6500\n",
      "7000\n",
      "7500\n",
      "8000\n",
      "8500\n",
      "9000\n",
      "9500\n",
      "end\n"
     ]
    }
   ],
   "source": [
    "@dataclass\n",
    "class Args:\n",
    "    env_id: str = 'CartPole-v1'\n",
    "    seed: int = 0\n",
    "    steps: int = 10000\n",
    "    lr: float = 1e-3\n",
    "    max_episode_len: int = 400\n",
    "    \n",
    "    writer: bool = True\n",
    "    experement_num: int = 0\n",
    "    log_path: str = f'runs\\experiment_{experement_num}'\n",
    "\n",
    "# args  = tyro.from_args(Args)\n",
    "args = Args()\n",
    "\n",
    "class Policy(nn.Module):\n",
    "    def __init__(self, env):\n",
    "        super().__init__()\n",
    "        self.linear1 = nn.Linear(env.observation_space.shape[0], 128)\n",
    "        self.linear2 = nn.Linear(128, 128)\n",
    "        self.linear3 = nn.Linear(128, env.action_space.n)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.linear1(x))\n",
    "        x = F.relu(self.linear2(x))\n",
    "        x = self.linear3(x)\n",
    "        x = F.softmax(x, dim=-1)\n",
    "        return x\n",
    "\n",
    "# Seeds\n",
    "np.random.seed(args.seed)\n",
    "torch.manual_seed(args.seed)\n",
    "\n",
    "#env\n",
    "env = gym.make(args.env_id)\n",
    "obs, _ = env.reset()\n",
    "#policy\n",
    "policy = Policy(env)\n",
    "optimizer = Adam(policy.parameters(), lr=args.lr)\n",
    "\n",
    "# results writer for tensorboard\n",
    "if args.writer:\n",
    "    writer = SummaryWriter(args.log_path)\n",
    "    # writer.add_text(args.text_id, args.exp_summary)\n",
    "    writer.add_graph(policy, torch.as_tensor(obs))\n",
    "#save reward sum for SummaryWriter\n",
    "    \n",
    "\n",
    "# #training algo\n",
    "# def generate_episode(env, policy):\n",
    "#     obs, _ = env.reset()\n",
    "#     log_probs, rewards = [], []\n",
    "#     done = False\n",
    "#     while not done:\n",
    "\n",
    "#         probs = policy(torch.as_tensor(obs))\n",
    "    \n",
    "#         cat = Categorical(probs)\n",
    "#         action = cat.sample()\n",
    "#         log_prob = cat.log_prob(action)\n",
    "\n",
    "#         new_obs, reward, done, truncated, info = env.step(action.item())\n",
    "#         # print(new_obs, reward)\n",
    "#         if truncated:\n",
    "#             print(done, truncated)\n",
    "        \n",
    "#         log_probs.append(log_prob)\n",
    "#         rewards.append(reward)\n",
    "\n",
    "#         obs = new_obs\n",
    "        \n",
    "#     return torch.stack(log_probs), torch.as_tensor(np.asarray(rewards))\n",
    "\n",
    "\n",
    "# def train(steps, env, policy):\n",
    "#     for step in range(steps):\n",
    "#         if step % 500 == 0:\n",
    "#             print(step)\n",
    "#         log_probs, rewards = generate_episode(env, policy)\n",
    "\n",
    "#         if args.writer:\n",
    "#             writer.add_scalar('episodic_return', rewards.sum(), step)\n",
    "#             writer.add_scalar('episodic_lenght', log_probs.shape[0], step)\n",
    "        \n",
    "#         loss = - (log_probs * rewards).mean()\n",
    "\n",
    "#         optimizer.zero_grad()\n",
    "#         loss.backward()\n",
    "#         optimizer.step()\n",
    "#===================================================================================\n",
    "#===================================================================================\n",
    "#===================================================================================\n",
    "\n",
    "def discount_rewards(rewards, gamma=0.99):\n",
    "    discounted = []\n",
    "    r = 0\n",
    "    for reward in reversed(rewards):\n",
    "        r = reward + gamma * r\n",
    "        discounted.insert(0, r)\n",
    "    return torch.as_tensor(discounted)\n",
    "\n",
    "def generate_episode(env, policy):\n",
    "    obs, _ = env.reset()\n",
    "    log_probs, rewards = [], []\n",
    "    done, truncated = False, False\n",
    "    ep_len = 0\n",
    "    while not done and not truncated and ep_len <  args.max_episode_len:  # Исправлено\n",
    "        ep_len += 1\n",
    "        \n",
    "        probs = policy(torch.as_tensor(obs, dtype=torch.float32))\n",
    "        cat = Categorical(probs)\n",
    "        action = cat.sample()\n",
    "        log_prob = cat.log_prob(action)\n",
    "\n",
    "        new_obs, reward, done, truncated, info = env.step(action.item())\n",
    "        \n",
    "        log_probs.append(log_prob)\n",
    "        rewards.append(reward)\n",
    "\n",
    "        obs = new_obs\n",
    "\n",
    "    rewards = discount_rewards(rewards)  # Добавлено дисконтирование\n",
    "    return torch.stack(log_probs), rewards\n",
    "\n",
    "def train(steps, env, policy):\n",
    "    for step in range(steps):\n",
    "        if step % 500 == 0:\n",
    "            print(step)\n",
    "        log_probs, rewards = generate_episode(env, policy)\n",
    "        # print(log_probs.shape)\n",
    "\n",
    "        if args.writer:\n",
    "            # print('reward: shape | sum: ', rewards.shape, rewards.sum())\n",
    "            writer.add_scalar('episodic_return', rewards.sum(), step)\n",
    "            # print('log_probs: shape: ', log_probs.shape[0])\n",
    "            writer.add_scalar('episodic_lenght', log_probs.shape[0], step)\n",
    "\n",
    "        baseline = rewards.mean()  # Добавлено baseline\n",
    "        loss = - (log_probs * (rewards - baseline)).mean()  # Исправлено\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "\n",
    "train(args.steps, env, policy)\n",
    "print('end')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2a99ec76-84ae-4abc-b7ae-764cadf809c4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The tensorboard extension is already loaded. To reload it, use:\n",
      "  %reload_ext tensorboard\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Reusing TensorBoard on port 6007 (pid 3444), started 2 days, 2:56:54 ago. (Use '!kill 3444' to kill it.)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "      <iframe id=\"tensorboard-frame-8435a65d1f93a646\" width=\"100%\" height=\"800\" frameborder=\"0\">\n",
       "      </iframe>\n",
       "      <script>\n",
       "        (function() {\n",
       "          const frame = document.getElementById(\"tensorboard-frame-8435a65d1f93a646\");\n",
       "          const url = new URL(\"/\", window.location);\n",
       "          const port = 6007;\n",
       "          if (port) {\n",
       "            url.port = port;\n",
       "          }\n",
       "          frame.src = url;\n",
       "        })();\n",
       "      </script>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%load_ext tensorboard\n",
    "%tensorboard --logdir runs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "04a5799b-4115-4a63-8684-9d08c521d818",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(policy.state_dict(), 'weights.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "8a469650-21de-4b78-990d-e1eae412ee82",
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make(args.env_id, render_mode='rgb_array')\n",
    "obs, _ = env.reset()\n",
    "done = False\n",
    "while not done:\n",
    "    # Преобразуйте состояние в тензор\n",
    "    obs_tensor = torch.as_tensor(obs, dtype=torch.float32)\n",
    "    \n",
    "    # Получите вероятности действий\n",
    "    with torch.no_grad():\n",
    "        probs = policy(obs_tensor)\n",
    "    \n",
    "    # Создайте распределение и выберите действие\n",
    "    cat = Categorical(probs)\n",
    "    action = cat.sample()\n",
    "    \n",
    "    # Выполните действие в среде\n",
    "    obs, reward, done, _, _ = env.step(action.item())\n",
    "    # Отрендерите среду\n",
    "    env.render()\n",
    "\n",
    "# Закройте среду после завершения\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "006af0c6-80dd-4e5c-859b-852a43e1b7c7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
