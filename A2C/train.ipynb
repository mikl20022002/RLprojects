{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "dc9e2a7b-6af2-456f-b592-1194f9ae5a5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.nn import functional as F\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from torch.distributions import Categorical\n",
    "from torch.optim import Adam\n",
    "import tensorboard\n",
    "from dataclasses import dataclass\n",
    "# from stable_baselines3.common.buffers\n",
    "import gymnasium as gym\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "23bbb277-f322-4ab8-bd7b-6b40a0ae9ace",
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class Args:\n",
    "    env_id = 'CartPole-v1'\n",
    "    seed = 0\n",
    "    num_vec_envs = 2\n",
    "    device = 'cude' if torch.cuda.is_available() else 'cpu'\n",
    "    gamma = 0.9\n",
    "    lr = 1e-4\n",
    "    critic_coeff = 0.5\n",
    "    entropy_coeff = 0.01\n",
    "\n",
    "    total_steps = 10000\n",
    "    steps_per_episode = 100\n",
    "    num_episodes = total_steps // steps_per_episode\n",
    "    \n",
    "    writer = True\n",
    "    log_dir = 'runs'\n",
    "\n",
    "args = Args()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "50776ffc-0059-499b-991c-667ca29f45cb",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'SyncVectorEnv' object has no attribute 'reset_done'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[65], line 116\u001b[0m\n\u001b[0;32m    114\u001b[0m policy \u001b[38;5;241m=\u001b[39m Policy(envs)\n\u001b[0;32m    115\u001b[0m optimizer \u001b[38;5;241m=\u001b[39m Adam(policy\u001b[38;5;241m.\u001b[39mparameters(), lr\u001b[38;5;241m=\u001b[39margs\u001b[38;5;241m.\u001b[39mlr)\n\u001b[1;32m--> 116\u001b[0m \u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpolicy\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    117\u001b[0m torch\u001b[38;5;241m.\u001b[39msave(policy\u001b[38;5;241m.\u001b[39mstate_dict(), \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmodel_weights\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "Cell \u001b[1;32mIn[65], line 77\u001b[0m, in \u001b[0;36mtrain\u001b[1;34m(args, policy, optimizer)\u001b[0m\n\u001b[0;32m     75\u001b[0m     obs \u001b[38;5;241m=\u001b[39m next_obs\n\u001b[0;32m     76\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m done\u001b[38;5;241m.\u001b[39many() \u001b[38;5;129;01mor\u001b[39;00m truncated\u001b[38;5;241m.\u001b[39many():\n\u001b[1;32m---> 77\u001b[0m         obs, _ \u001b[38;5;241m=\u001b[39m \u001b[43menvs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreset_done\u001b[49m()\n\u001b[0;32m     79\u001b[0m \u001b[38;5;66;03m#get value for the final step\u001b[39;00m\n\u001b[0;32m     80\u001b[0m obs \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mtensor(np\u001b[38;5;241m.\u001b[39masarray(obs), device\u001b[38;5;241m=\u001b[39margs\u001b[38;5;241m.\u001b[39mdevice)\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'SyncVectorEnv' object has no attribute 'reset_done'"
     ]
    }
   ],
   "source": [
    "class Policy(nn.Module):\n",
    "    def __init__(self, envs):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.actor_linear1 = nn.Linear(envs.single_observation_space.shape[0], 32)\n",
    "        self.actor_linear2 = nn.Linear(32, int(envs.single_action_space.n))\n",
    "\n",
    "        self.critic_linear1 = nn.Linear(envs.single_observation_space.shape[0], 32)\n",
    "        self.critic_linear2 = nn.Linear(32, 1)\n",
    "\n",
    "    def actor(self, x):\n",
    "        x = F.relu( self.actor_linear1(x) )\n",
    "        x = F.softmax( self.actor_linear2(x), dim=-1 )\n",
    "        return x\n",
    "\n",
    "    def critic(self, x):\n",
    "        x = F.relu( self.critic_linear1(x) )\n",
    "        x = self.critic_linear2(x)\n",
    "        return x\n",
    "\n",
    "    def train_actor(self, x):\n",
    "        probs = self.actor(x)\n",
    "        cat = Categorical(probs)\n",
    "        actions = cat.sample()\n",
    "        log_probs = cat.log_prob(actions)\n",
    "        entropy = cat.entropy()\n",
    "        return actions, log_probs, entropy\n",
    "\n",
    "\n",
    "def make_env(env_id, seed):\n",
    "    def _make():\n",
    "        env = gym.make(env_id)\n",
    "        # env.seed(seed + index)\n",
    "        return env\n",
    "    return _make\n",
    "\n",
    "\n",
    "def make_vec_envs(num_vecs):\n",
    "    envs = gym.vector.SyncVectorEnv(\n",
    "        [make_env(args.env_id, args.seed + i) for i in range(num_vecs)]\n",
    "    )\n",
    "    return envs\n",
    "\n",
    "\n",
    "def calc_return(rewards, final_value, gamma):\n",
    "    rewards.append(final_value)\n",
    "    for i in reversed(range(len(rewards) - 1)):\n",
    "        # print('return at state ', i)\n",
    "        # print(rewards[i], \" + \", rewards[i+1], \" * \", gamma)\n",
    "        rewards[i] += rewards[i + 1] * gamma\n",
    "    rewards.pop()\n",
    "    return np.stack(rewards)\n",
    "\n",
    "\n",
    "def train(args, policy, optimizer):\n",
    "    if args.writer:\n",
    "        writer = SummaryWriter(args.log_dir)\n",
    "        \n",
    "    for ep in range(args.num_episodes):\n",
    "        obs, _ = envs.reset()\n",
    "        values, log_probs, rewards, entropys = [], [], [], []\n",
    "        for step in range(args.steps_per_episode):\n",
    "            obs = torch.tensor(np.asarray(obs), device=args.device)\n",
    "            actions, log_prob, entropy = policy.train_actor(obs)\n",
    "            value = policy.critic(obs)\n",
    "            \n",
    "            next_obs, reward, done, truncated, info = envs.step(actions.numpy())\n",
    "            \n",
    "            log_probs.append(log_prob)\n",
    "            values.append(value)\n",
    "            rewards.append(reward)\n",
    "            entropys.append(entropy)\n",
    "\n",
    "            # restart envs if they finished\n",
    "            obs = next_obs\n",
    "            if done.any() or truncated.any():\n",
    "                obs, _ = envs.reset_done()\n",
    "\n",
    "        #get value for the final step\n",
    "        obs = torch.tensor(np.asarray(obs), device=args.device)\n",
    "        with torch.no_grad():\n",
    "            final_value = policy.critic(obs)\n",
    "\n",
    "        #calc episodic return\n",
    "        ep_return = calc_return(rewards, final_value.squeeze().numpy(), args.gamma)\n",
    "        \n",
    "        if arge.writer:\n",
    "            writer.add_scalar('episodic return', ep_return, ep)\n",
    "            \n",
    "        #make tensors\n",
    "        ep_return = torch.as_tensor(ep_return, device=args.device)\n",
    "        log_probs = torch.stack(log_probs)\n",
    "        values = torch.stack(values).squeeze()\n",
    "        entropys = torch.stack(entropys)\n",
    "        # print(type(entropy), entropy.shape)\n",
    "\n",
    "        #calc advantage\n",
    "        advantages = ep_return - values\n",
    "\n",
    "        #calc actor, critic and total losses\n",
    "        critic_loss = advantages.pow(2).mean()\n",
    "        actor_loss = - (advantages * log_probs).mean()\n",
    "        entropy_loss = entropys.mean()\n",
    "        loss = (actor_loss + args.critic_coeff * critic_loss - args.entropy_coeff * entropy_loss)\n",
    "\n",
    "        #train step\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "\n",
    "envs = make_vec_envs(args.num_vec_envs)\n",
    "\n",
    "policy = Policy(envs)\n",
    "optimizer = Adam(policy.parameters(), lr=args.lr)\n",
    "train(args, policy, optimizer)\n",
    "torch.save(policy.state_dict(), 'model_weights')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32d60d06-4273-423d-be68-a971eb1c0506",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
